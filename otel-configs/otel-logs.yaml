# OpenTelemetry Collector Logs Configuration
# Simplified configuration for collecting logs from cronjob-scale-down-operator-controller-manager
# and sending them to Grafana Cloud via OTLP

# Use DaemonSet mode for log collection from all nodes
mode: daemonset

# Completely disable all presets since we want a custom minimal configuration
presets:
  logsCollection:
    enabled: false
  kubernetesAttributes:
    enabled: false
  kubeletMetrics:
    enabled: false
  kubernetesEvents:
    enabled: false
  hostMetrics:
    enabled: false

# OpenTelemetry Collector image configuration
image:
  repository: otel/opentelemetry-collector-contrib
  tag: "0.131.1"
  pullPolicy: IfNotPresent

# Service configuration for logs collector
service:
  type: ClusterIP

# Disable all default ports, only enable what we need
ports:
  otlp:
    enabled: false
  otlp-http:
    enabled: false  # We'll configure this in our custom config
  jaeger-compact:
    enabled: false
  jaeger-thrift:
    enabled: false
  jaeger-grpc:
    enabled: false
  zipkin:
    enabled: false
  metrics:
    enabled: false

# Resource configuration optimized for log processing
resources:
  limits:
    cpu: 300m
    memory: 512Mi
  requests:
    cpu: 50m
    memory: 128Mi

# Volume mounts for accessing container logs
extraVolumes:
  - name: varlogpods
    hostPath:
      path: /var/log/pods
  - name: varlibdockercontainers
    hostPath:
      path: /var/lib/docker/containers

extraVolumeMounts:
  - name: varlogpods
    mountPath: /var/log/pods
    readOnly: true
  - name: varlibdockercontainers
    mountPath: /var/lib/docker/containers
    readOnly: true

# Tolerations to run on all nodes including control plane
tolerations:
  - operator: Exists
    effect: NoSchedule
  - operator: Exists
    effect: NoExecute

# OpenTelemetry Collector configuration for logs
config:
  receivers:
    # Filelog receiver for Kubernetes container logs - only cronjob-scale-down-operator
    filelog:
      include:
        - /var/log/pods/default_cronjob-scale-down-operator-controller-manager*/*/*.log
      start_at: end
      include_file_path: true
      include_file_name: false
      operators:
        # Parse container logs that are in JSON format (outer container format)
        - type: json_parser
          id: parser-docker
          output: extract_metadata_from_filepath
          timestamp:
            parse_from: attributes.time
            layout: '%Y-%m-%dT%H:%M:%S.%LZ'
        
        # Extract metadata from file path
        - type: regex_parser
          id: extract_metadata_from_filepath
          regex: '^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]{36})\/(?P<container_name>[^\._]+)\/(?P<restart_count>\d+)\.log$'
          parse_from: attributes["log.file.path"]
          output: parse_log_message
        
        # Parse the inner log message structure (timestamp, level, message, JSON)
        - type: regex_parser
          id: parse_log_message
          regex: '^(?P<timestamp>\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}Z)\t(?P<level>\w+)\t(?P<message>[^\t]*)\t(?P<json_data>\{.*\})?\n?$'
          parse_from: attributes.log
          output: parse_json_data
          on_error: send
        
        # Parse the JSON data within the log message
        - type: json_parser
          id: parse_json_data
          parse_from: attributes.json_data
          output: extract_controller_info
          on_error: send
        
        # Extract and restructure controller-specific fields
        - type: move
          id: extract_controller_info
          from: attributes.controller
          to: attributes["k8s.controller.name"]
          output: extract_controller_group
          on_error: send
        
        - type: move
          id: extract_controller_group
          from: attributes.controllerGroup
          to: attributes["k8s.controller.group"]
          output: extract_controller_kind
          on_error: send
        
        - type: move
          id: extract_controller_kind
          from: attributes.controllerKind
          to: attributes["k8s.controller.kind"]
          output: extract_reconcile_id
          on_error: send
        
        - type: move
          id: extract_reconcile_id
          from: attributes.reconcileID
          to: attributes["reconcile.id"]
          output: extract_requeue_after
          on_error: send
        
        - type: move
          id: extract_requeue_after
          from: attributes.requeueAfter
          to: attributes["reconcile.requeue_after"]
          output: set_log_level
          on_error: send
        
        # Set the log level from parsed data
        - type: move
          id: set_log_level
          from: attributes.level
          to: attributes["log.level"]
          output: set_log_message
          on_error: send
        
        # Set the actual log message
        - type: move
          id: set_log_message
          from: attributes.message
          to: attributes["log.message"]
          output: set_timestamp
          on_error: send
        
        # Use the parsed timestamp as the log timestamp
        - type: time_parser
          id: set_timestamp
          parse_from: attributes.timestamp
          layout: '%Y-%m-%dT%H:%M:%SZ'
          output: add_source
          on_error: send
        
        # Add source information
        - type: add
          id: add_source
          field: attributes["log.source"]
          value: "cronjob-scale-down-operator"

  processors:
    # Batch processor for performance
    batch:
      timeout: 1s
      send_batch_size: 1024
      send_batch_max_size: 2048
    
    # Memory limiter
    memory_limiter:
      limit_mib: 400
      spike_limit_mib: 100
      check_interval: 5s
    
    # Resource processor to add metadata
    resource:
      attributes:
        - key: service.name
          value: "otel-logs-collector"
          action: upsert
        - key: service.version
          value: "1.0.0"
          action: upsert
        - key: k8s.cluster.name
          value: "${K8S_CLUSTER_NAME}"
          action: upsert
    
    # Kubernetes attributes processor
    k8sattributes:
      auth_type: serviceAccount
      passthrough: false
      filter:
        node_from_env_var: KUBE_NODE_NAME
      extract:
        metadata:
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.deployment.name
          - k8s.namespace.name
          - k8s.node.name
          - k8s.pod.start_time
  exporters:
    # OTLP exporter to Grafana Cloud Loki (using HTTP)
    otlphttp/grafana-logs:
      endpoint: "${GRAFANA_LOKI_OTLP_ENDPOINT}"
      headers:
        authorization: "Basic ${GRAFANA_AUTH_TOKEN}"
      compression: gzip
      timeout: 30s
      retry_on_failure:
        enabled: true
        initial_interval: 5s
        max_interval: 30s
        max_elapsed_time: 300s
      sending_queue:
        enabled: true
        num_consumers: 2
        queue_size: 50

  extensions:
    # Health check
    health_check:
      endpoint: 0.0.0.0:13133

  service:
    extensions: [health_check]
    pipelines:
      # Single logs pipeline for cronjob-scale-down-operator
      logs:
        receivers: [filelog]
        processors: [memory_limiter, k8sattributes, resource, batch]
        exporters: [otlphttp/grafana-logs]
    
    telemetry:
      logs:
        level: "info"

# Environment variables
extraEnvs:
  - name: KUBE_NODE_NAME
    valueFrom:
      fieldRef:
        apiVersion: v1
        fieldPath: spec.nodeName
  - name: K8S_CLUSTER_NAME
    value: "your-cluster-name"
  - name: GRAFANA_LOKI_OTLP_ENDPOINT
    valueFrom:
      secretKeyRef:
        name: grafana-cloud-config
        key: loki-otlp-endpoint
  - name: GRAFANA_AUTH_TOKEN
    valueFrom:
      secretKeyRef:
        name: grafana-cloud-config
        key: auth-token

# Service account with proper RBAC for Kubernetes access
serviceAccount:
  create: true

# RBAC for accessing Kubernetes API
clusterRole:
  create: true
  rules:
    - apiGroups: [""]
      resources: ["pods", "namespaces"]
      verbs: ["get", "list", "watch"]

# Security context
securityContext:
  runAsUser: 0  # Required for reading log files